{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhNlCH6P+C+OIFuysAv3Yq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitgit991/Depression-Detection-/blob/main/CNN%20Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q21wMr4tfnKM",
        "outputId": "59ed5e61-30c9-431f-ee35-2d3f58ffab17"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWXXuLXKgPGS",
        "outputId": "fbd26710-d56e-46ab-e46b-ec8395d696f5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "import os\n",
        "\n",
        "# Change directory to your dataset location\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "import gc\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from smart_open import open\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.layers import Dropout\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rBEdGr1gg6p",
        "outputId": "14f8f8af-5b22-4941-bfac-3167580cd6a3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "dataset1 = np.array(pd.read_csv('/content/drive/My Drive/MCA Dataset/dev_split_Depression_AVEC2017.csv', delimiter=',', encoding='utf-8'))[:, 0:2]\n",
        "dataset2 = np.array(pd.read_csv('/content/drive/My Drive/MCA Dataset/full_test_split.csv', delimiter=',', encoding='utf-8'))[:, 0:2]\n",
        "dataset3 = np.array(pd.read_csv('/content/drive/My Drive/MCA Dataset/train_split_Depression_AVEC2017.csv', delimiter=',', encoding='utf-8'))[:, 0:2]\n",
        "\n",
        "dataset = np.concatenate((dataset1, np.concatenate((dataset2, dataset3))))\n",
        "\n",
        "def checkPosNeg(dataset, index):\n",
        "    for i in range(len(dataset)):\n",
        "        if dataset[i][0] == index:\n",
        "            return dataset[i][1]\n",
        "    return 0\n",
        "\n",
        "# Initialize lists\n",
        "Data, Y = [], []\n",
        "Data_test, Y_test = [], []\n",
        "\n",
        "# Process training data\n",
        "for i in range(len(dataset3)):\n",
        "    val = checkPosNeg(dataset, dataset3[i][0])\n",
        "    Y.append(val)\n",
        "    try:\n",
        "        fileName = f\"/content/drive/My Drive/MCA Dataset/train_data/{int(dataset3[i][0])}_TRANSCRIPT.csv\"\n",
        "        Data.append(np.array(pd.read_csv(fileName, delimiter='\\t', encoding='utf-8', engine='python'))[:, 2:4])\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "# Process validation data\n",
        "for i in range(len(dataset1)):\n",
        "    val = checkPosNeg(dataset, dataset1[i][0])\n",
        "    Y.append(val)\n",
        "    try:\n",
        "        fileName = f\"/content/drive/My Drive/MCA Dataset/dev_data/{int(dataset1[i][0])}_TRANSCRIPT.csv\"\n",
        "        Data.append(np.array(pd.read_csv(fileName, delimiter='\\t', encoding='utf-8', engine='python'))[:, 2:4])\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "# Process test data\n",
        "for i in range(len(dataset2)):\n",
        "    Y_test.append(checkPosNeg(dataset, dataset2[i][0]))\n",
        "    try:\n",
        "        fileName = f\"/content/drive/My Drive/MCA Dataset/test_data/{int(dataset2[i][0])}_TRANSCRIPT.csv\"\n",
        "        Data_test.append(np.array(pd.read_csv(fileName, delimiter='\\t', encoding='utf-8', engine='python'))[:, 2:4])\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "Y = np.array(Y)\n",
        "Y_test = np.array(Y_test)\n",
        "\n",
        "Data2, Data2_test = [], []\n",
        "\n",
        "# Extract participant's speech\n",
        "for i in range(len(Data)):\n",
        "    script = [Data[i][k][1] for k in range(1, len(Data[i])) if Data[i][k][0] == \"Participant\"]\n",
        "    Data2.append(script)\n",
        "\n",
        "for i in range(len(Data_test)):\n",
        "    script = [Data_test[i][k][1] for k in range(1, len(Data_test[i])) if Data_test[i][k][0] == \"Participant\"]\n",
        "    Data2_test.append(script)\n",
        "\n",
        "# Free memory\n",
        "Data, Data_test = [], []\n",
        "gc.collect()\n",
        "\n",
        "# Convert to numpy object arrays\n",
        "Data2 = np.array(Data2, dtype=object)\n",
        "Data2_test = np.array(Data2_test, dtype=object)\n",
        "\n",
        "# Load word embeddings\n",
        "model = KeyedVectors.load_word2vec_format('/content/drive/My Drive/MCA Dataset/GoogleNews-vectors-negative300.bin', binary=True)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_StopWords(sentence):\n",
        "    return [w for w in sentence if w not in stop_words]\n",
        "\n",
        "# Create feature matrices\n",
        "max_num_words, max_num_sentence = 20, 250\n",
        "finalMatrix = np.zeros((len(Data2), max_num_sentence, max_num_words, 300))\n",
        "finalMatrix_test = np.zeros((len(Data2_test), max_num_sentence, max_num_words, 300))\n",
        "\n",
        "# Convert words to embeddings for training data\n",
        "for k in range(len(Data2)):\n",
        "    for i in range(min(max_num_sentence, len(Data2[k]))):\n",
        "        try:\n",
        "            sentence = str(Data2[k][i])  # Convert to string to avoid float error\n",
        "            sentence = remove_StopWords(sentence.split(\" \"))  # Remove stopwords\n",
        "            for j in range(min(max_num_words, len(sentence))):\n",
        "                try:\n",
        "                    word = sentence[j].strip(\"<>\").strip()  # Clean word\n",
        "                    finalMatrix[k][i][j] = model[word]\n",
        "                except KeyError:\n",
        "                    continue  # Ignore words not found in embeddings\n",
        "        except Exception as e:\n",
        "            continue  # Skip problematic rows\n",
        "\n",
        "# Convert words to embeddings for test data\n",
        "for k in range(len(Data2_test)):\n",
        "    for i in range(min(max_num_sentence, len(Data2_test[k]))):\n",
        "        try:\n",
        "            sentence = str(Data2_test[k][i])  # Convert to string\n",
        "            sentence = remove_StopWords(sentence.split(\" \"))\n",
        "            for j in range(min(max_num_words, len(sentence))):\n",
        "                try:\n",
        "                    word = sentence[j].strip(\"<>\").strip()\n",
        "                    finalMatrix_test[k][i][j] = model[word]\n",
        "                except KeyError:\n",
        "                    continue\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "\n",
        "# Free memory\n",
        "Data2, Data2_test, model, stop_words = [], [], [], []\n",
        "gc.collect()\n",
        "\n",
        "# Function to upsample data\n",
        "def upsample(X_train, Y_train):\n",
        "    X_train = np.array(X_train)\n",
        "    Y_train = np.array(Y_train)\n",
        "\n",
        "    print(\"Before upsampling: \", X_train.shape, Y_train.shape)\n",
        "\n",
        "    if len(X_train) != len(Y_train):\n",
        "        print(f\"Mismatch: X_train has {len(X_train)} samples, Y_train has {len(Y_train)} labels\")\n",
        "        Y_train = Y_train[:len(X_train)]  # Trim labels if needed\n",
        "\n",
        "    X_train_0 = X_train[Y_train == 0]\n",
        "    X_train_1 = X_train[Y_train == 1]\n",
        "\n",
        "    Y_train_1 = Y_train[Y_train == 1]\n",
        "    size = X_train_0.shape[0] - X_train_1.shape[0]\n",
        "\n",
        "    X_train, Y_train = list(X_train), list(Y_train)\n",
        "\n",
        "    while size > 0:\n",
        "        size -= 1\n",
        "        index = np.random.randint(0, X_train_1.shape[0] - 1)\n",
        "        leave_index = np.random.randint(0, len(X_train) - 1)\n",
        "        X_add, X_leave = X_train_1[index], X_train[leave_index]\n",
        "        Y_add, Y_leave = Y_train_1[index], Y_train[leave_index]\n",
        "\n",
        "        X_train[leave_index] = X_add\n",
        "        X_train.append(X_leave)\n",
        "        Y_train[leave_index] = Y_add\n",
        "        Y_train.append(Y_leave)\n",
        "\n",
        "    return np.array(X_train), np.array(Y_train)\n",
        "\n",
        "# **Fix shape mismatch before upsampling**\n",
        "if len(Y) != finalMatrix.shape[0]:\n",
        "    print(f\"⚠️ Warning: Shape mismatch - Adjusting Y from {len(Y)} to {finalMatrix.shape[0]}\")\n",
        "    Y = Y[:finalMatrix.shape[0]]\n",
        "\n",
        "if len(Y_test) != finalMatrix_test.shape[0]:\n",
        "    print(f\"⚠️ Warning: Shape mismatch - Adjusting Y_test from {len(Y_test)} to {finalMatrix_test.shape[0]}\")\n",
        "    Y_test = Y_test[:finalMatrix_test.shape[0]]\n",
        "\n",
        "# **Convert Y to numpy array**\n",
        "Y = np.array(Y, dtype=np.int32)\n",
        "Y_test = np.array(Y_test, dtype=np.int32)\n",
        "\n",
        "# **Run upsampling**\n",
        "finalMatrix, Y = upsample(finalMatrix, Y)\n",
        "finalMatrix_test, Y_test = upsample(finalMatrix_test, Y_test)\n",
        "\n",
        "print(\"Final shape after upsampling:\")\n",
        "print(\"Train:\", finalMatrix.shape, Y.shape)\n",
        "print(\"Test:\", finalMatrix_test.shape, Y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vx_uNGeglprj",
        "outputId": "f73e44e8-c481-4df4-cd05-b171261b7f68"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Warning: Shape mismatch - Adjusting Y from 142 to 37\n",
            "⚠️ Warning: Shape mismatch - Adjusting Y_test from 47 to 14\n",
            "Before upsampling:  (37, 250, 20, 300) (37,)\n",
            "Before upsampling:  (14, 250, 20, 300) (14,)\n",
            "Final shape after upsampling:\n",
            "Train: (40, 250, 20, 300) (40,)\n",
            "Test: (16, 250, 20, 300) (16,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input\n",
        "\n",
        "class CNN_Text:\n",
        "    def __init__(self):\n",
        "        classifier = Sequential()\n",
        "        classifier.add(Input(shape=(finalMatrix.shape[1], finalMatrix.shape[2], finalMatrix.shape[3])))\n",
        "        classifier.add(Conv2D(150, (1, 5), activation='relu'))\n",
        "        classifier.add(MaxPooling2D(pool_size=(1, 3)))\n",
        "        classifier.add(Conv2D(75, (1, 3), activation='relu'))\n",
        "        classifier.add(MaxPooling2D(pool_size=(1, 2)))\n",
        "        classifier.add(Flatten())\n",
        "        classifier.add(Dense(units=128, activation='relu'))\n",
        "        classifier.add(Dense(units=1, activation='sigmoid'))\n",
        "        classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "        self.classifier = classifier  # Corrected spelling\n",
        "\n",
        "    def fitModel(self, X, Y, epoch=10):\n",
        "        class_weight = {0: 0.5, 1: 0.6}\n",
        "        self.classifier.fit(X, Y, epochs=epoch, class_weight=class_weight)\n",
        "\n",
        "    def predictModel(self, X):\n",
        "        return Thresholding(self.classifier.predict(X), 0.5)\n",
        "\n",
        "# Initialize and train model\n",
        "model = CNN_Text()\n",
        "model.fitModel(finalMatrix, Y, 10)\n",
        "Y_Pred = model.predictModel(finalMatrix_test)\n",
        "print(classification_report(Y_test, Y_Pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LU9Rakv3mdaW",
        "outputId": "898f557c-95bd-407e-8f58-b8fcbeed85f8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 751ms/step - accuracy: 0.5000 - loss: 0.3899\n",
            "Epoch 2/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.2457\n",
            "Epoch 3/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 705ms/step - accuracy: 1.0000 - loss: 0.1146\n",
            "Epoch 4/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 717ms/step - accuracy: 1.0000 - loss: 0.0402\n",
            "Epoch 5/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0176\n",
            "Epoch 6/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 699ms/step - accuracy: 1.0000 - loss: 0.0040\n",
            "Epoch 7/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0018\n",
            "Epoch 8/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 726ms/step - accuracy: 1.0000 - loss: 2.8257e-04\n",
            "Epoch 9/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 1.0630e-04\n",
            "Epoch 10/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 740ms/step - accuracy: 1.0000 - loss: 8.8519e-05\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 715ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.88      0.74         8\n",
            "           1       0.80      0.50      0.62         8\n",
            "\n",
            "    accuracy                           0.69        16\n",
            "   macro avg       0.72      0.69      0.68        16\n",
            "weighted avg       0.72      0.69      0.68        16\n",
            "\n"
          ]
        }
      ]
    }
  ]
}